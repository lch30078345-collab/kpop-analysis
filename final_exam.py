import streamlit as st
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import altair as alt
import plotly.express as px
import networkx as nx
import urllib.request
import json
import re
from konlpy.tag import Okt
from wordcloud import WordCloud
from collections import Counter
from itertools import combinations

# 1. í˜ì´ì§€ ë° í°íŠ¸ 
st.set_page_config(page_title="KíŒ ë°ëª¬ í—Œí„°ìŠ¤ ë¶„ì„", layout="wide")


font_path = "Pretendard-Regular.ttf"
stopwords_path = "stopwords.txt"

fm.fontManager.addfont(font_path)
font_name = fm.FontProperties(fname=font_path).get_name()
plt.rc('font', family=font_name)
plt.rcParams['axes.unicode_minus'] = False

# 2. í•™ë²ˆ ë° ì´ë¦„ 
st.title("KíŒ ë°ëª¬ í—Œí„°ìŠ¤: íŒ¬ë¤ í˜•ì„±ì˜ í•µì‹¬ ìš”ì¸ ë¶„ì„")
st.subheader("í•™ë²ˆ : C321058  |  ì´ë¦„ : ì´ì±„í¬")
st.markdown("""
> **ê¸°íš ì˜ë„:** ë³¸ ëŒ€ì‹œë³´ë“œëŠ” 2025ë…„ í™”ì œì‘ 'KíŒ ë°ëª¬ í—Œí„°ìŠ¤'ì— ëŒ€í•œ ì˜¨ë¼ì¸ ì—¬ë¡ ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ìˆ˜ì§‘í•˜ì—¬ ë¶„ì„í•©ë‹ˆë‹¤.  
> ë‹¨ìˆœí•œ ì–¸ê¸‰ëŸ‰ì„ ë„˜ì–´ì„œ, ëŒ€ì¤‘ì´ ì–´ë–¤ í‚¤ì›Œë“œì— ë°˜ì‘í•˜ê³  ìˆìœ¼ë©°, í‚¤ì›Œë“œ ê°„ì— ì–´ë–¤ ì—°ê²°ì„±ì´ ìˆëŠ”ì§€ íŒŒì•…í•˜ì—¬ íŒ¬ë¤ í˜•ì„±ì˜ ì›ë™ë ¥ì„ ë„ì¶œí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.
""")
st.write("---")

# 3. ì‚¬ì´ë“œë°” ìœ„ì ¯ êµ¬ì„±
st.sidebar.header("Step 1. ë°ì´í„° ìˆ˜ì§‘ ì„¤ì •")
query = st.sidebar.text_input("ê²€ìƒ‰ì–´ ì…ë ¥", "KíŒ ë°ëª¬ í—Œí„°ìŠ¤")
display_count = st.sidebar.slider("ìˆ˜ì§‘í•  ê¸°ì‚¬ ìˆ˜", 10, 100, 100, 10)
sort_option = st.sidebar.selectbox("ì •ë ¬ ê¸°ì¤€", ["sim", "date"])
collect_btn = st.sidebar.button("ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ ì‹œì‘")

st.sidebar.divider()
st.sidebar.header("Step 2. ì‹œê°í™” ì˜µì…˜")
wc_bg = st.sidebar.radio("ì›Œë“œí´ë¼ìš°ë“œ ë°°ê²½", ["white", "black"], horizontal=True)
min_edge = st.sidebar.slider("ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìµœì†Œ ë¹ˆë„", 1, 15, 3)

# 4. ë°ì´í„° ìˆ˜ì§‘ 
if collect_btn:
    client_id = "BAa7WmdQwBpItekevUgc"
    client_secret = "BRRmddBdNS"
    
    encText = urllib.parse.quote(query)
    url = f"https://openapi.naver.com/v1/search/news.json?query={encText}&display={display_count}&sort={sort_option}"
    
    request = urllib.request.Request(url)
    request.add_header("X-Naver-Client-Id", client_id)
    request.add_header("X-Naver-Client-Secret", client_secret)
    
    response = urllib.request.urlopen(request)
    if response.getcode() == 200:
        data = json.loads(response.read().decode('utf-8'))
        df = pd.DataFrame(data['items'])
        
        df['pubDate'] = pd.to_datetime(df['pubDate']).dt.date
        df['title'] = df['title'].str.replace('<b>', '').str.replace('</b>', '').str.replace('&quot;', '')
        df['description'] = df['description'].str.replace('<b>', '').str.replace('</b>', '').str.replace('&quot;', '')
        
        df.to_csv('collected_data.csv', index=False, encoding='utf-8-sig')
        st.session_state.df = df

# 5. ë¶„ì„ ë° ì‹œê°í™”
if "df" in st.session_state and st.session_state.df is not None:
    df = st.session_state.df
    
    # [ë°ì´í„° í™•ì¸]
    st.subheader("1. ë°ì´í„° ìˆ˜ì§‘ í˜„í™©")
    st.dataframe(df.head())
    st.info(f"**Data Insight:** ì´ {len(df)}ê±´ì˜ ìµœì‹  ê¸°ì‚¬ê°€ ìˆ˜ì§‘ë˜ì—ˆìŠµë‹ˆë‹¤. í•µì‹¬ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.")

    okt = Okt()
    
    # ë¶ˆìš©ì–´ ì²˜ë¦¬ (íŒŒì¼ + ì§ì ‘ ì¶”ê°€)
    stopwords = []
    try:
        with open(stopwords_path, 'r', encoding='utf-8') as f:
            stopwords = f.read().splitlines()
    except:
        pass 

    # ë‰´ìŠ¤ ê¸°ì‚¬ ì¡ìŒ ì œê±°ìš© ë¶ˆìš©ì–´ ì¶”ê°€
    extra_stopwords = ['ë‰´ìŠ¤', 'ê¸°ì‚¬', 'ê¸°ì', 'ì§€ë‚œ', 'ìœ„í•´', 'í†µí•´', 'ê´€ë ¨', 'ëŒ€í•œ', 'ê²½ìš°', 'ê°€ì¥', 'ì´ë²ˆ', 'ë•Œë¬¸', 'ì •ë„', 'ëŒ€í•´', 'ë¬´ë‹¨', 'ë°°í¬', 'ê¸ˆì§€', 'ì „ì¬', 'ì†ë³´', 'ì˜¤ëŠ˜', 'ë“±', 'ë°', 'ë°”','ìŠ¤ì¼€','ì œì´ì§€','ë§¤ê¸°']
    stopwords.extend(extra_stopwords)
    stopwords = list(set(stopwords)) # ì¤‘ë³µ ì œê±°
    
    all_nouns = []
    sentences = []
    text_data = df['title'] + " " + df['description']
    
    for text in text_data:
        clean_text = re.sub("[^ê°€-í£ ]", "", text)
        nouns = [n for n in okt.nouns(clean_text) if len(n) > 1 and n not in stopwords]
        all_nouns.extend(nouns)
        sentences.append(nouns)

    count = Counter(all_nouns)
    top_20 = pd.DataFrame(count.most_common(20), columns=['ë‹¨ì–´', 'ë¹ˆë„'])

    st.write("---")
    
    # Seaborn
    st.subheader("2. í•µì‹¬ ì´ìŠˆ í‚¤ì›Œë“œ (Seaborn)")
    fig1, ax1 = plt.subplots(figsize=(12, 6))
    sns.barplot(data=top_20, x='ë¹ˆë„', y='ë‹¨ì–´', ax=ax1, palette='viridis')
    st.pyplot(fig1)
    st.markdown("""
    **ğŸ‘‰ í•´ì„:** ë¹ˆë„ìˆ˜ê°€ ë†’ì€ ìƒìœ„ í‚¤ì›Œë“œë“¤ì€ í˜„ì¬ **ëŒ€ì¤‘ì´ ê°€ì¥ ì£¼ëª©í•˜ëŠ” ìš”ì†Œ**ì…ë‹ˆë‹¤. 
    íŠ¹ì • ë©¤ë²„ì˜ ì´ë¦„ì´ë‚˜ ê³¡ëª…, 'ë„·í”Œë¦­ìŠ¤' ë“±ì˜ í”Œë«í¼ ì´ë¦„ì´ ìƒìœ„ì— ìˆë‹¤ë©´ ê·¸ê²ƒì´ íŒ¬ë¤ ìœ ì…ì˜ ì£¼ ê²½ë¡œì„ì„ ì‹œì‚¬í•©ë‹ˆë‹¤.
    """)

    st.write("---")

    # Plotly
    st.subheader("3. í‚¤ì›Œë“œ ì ìœ ìœ¨ ë¶„ì„ (Plotly)")
    fig2 = px.pie(top_20.head(10), values='ë¹ˆë„', names='ë‹¨ì–´', hole=0.3, title="ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ ë¹„ì¤‘")
    st.plotly_chart(fig2, use_container_width=True)
    st.markdown("""
    **ğŸ‘‰ í•´ì„:** ìƒìœ„ 10ê°œ í‚¤ì›Œë“œê°€ ì „ì²´ ì´ìŠˆì—ì„œ ì°¨ì§€í•˜ëŠ” ë¹„ì¤‘ì…ë‹ˆë‹¤. 
    íŠ¹ì • í‚¤ì›Œë“œì˜ ë¹„ì¤‘ì´ ì••ë„ì ì´ë¼ë©´, íŒ¬ë¤ì˜ ê´€ì‹¬ì‚¬ê°€ **í•˜ë‚˜ì˜ ì´ìŠˆì— ì§‘ì¤‘**ë˜ì–´ ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
    """)

    st.write("---")

    # Altair
    st.subheader("4. ì‹œê³„ì—´ íŠ¸ë Œë“œ ë³€í™” (Altair)")
    trend_df = df['pubDate'].value_counts().reset_index()
    trend_df.columns = ['ë‚ ì§œ', 'ê¸°ì‚¬ìˆ˜']
    trend_df = trend_df.sort_values('ë‚ ì§œ')
    
    chart = alt.Chart(trend_df).mark_line(point=True, color='red').encode(
        x='ë‚ ì§œ', y='ê¸°ì‚¬ìˆ˜', tooltip=['ë‚ ì§œ', 'ê¸°ì‚¬ìˆ˜']
    ).interactive()
    
    st.altair_chart(chart, use_container_width=True)
    st.info("ğŸ‘‰ **Trend Insight:** ê·¸ë˜í”„ê°€ ê¸‰ê²©íˆ ìƒìŠ¹í•˜ëŠ” ì‹œì ì— ì£¼ìš” ì´ë²¤íŠ¸(í‹°ì €, ë°œë§¤ ë“±)ê°€ ìˆì—ˆëŠ”ì§€ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

    st.write("---")

    # WordCloud
    st.subheader("5. ì¢…í•© ì´ìŠˆ ì›Œë“œí´ë¼ìš°ë“œ")
    
    wc = WordCloud(
        font_path=font_path, 
        background_color='white',  
        width=900, 
        height=500, 
        colormap='cool',  
        max_words=30
    )
    gen = wc.generate_from_frequencies(dict(count.most_common(30)))
    
    fig3 = plt.figure(figsize=(12, 6))
    plt.imshow(gen)
    plt.axis('off')
    st.pyplot(fig3)
    st.markdown("**ğŸ‘‰ ìš”ì•½:** í…ìŠ¤íŠ¸ í¬ê¸°ê°€ í´ìˆ˜ë¡ íŒ¬ë¤ ë‚´ì—ì„œ ì–¸ê¸‰ëœ íšŸìˆ˜ê°€ ë§ì€ ì˜ë¯¸ ìˆëŠ” ë‹¨ì–´ì…ë‹ˆë‹¤.")
    st.write("---")

    # NetworkX
    st.subheader("6. í‚¤ì›Œë“œ ë™ì‹œì¶œí˜„ ë„¤íŠ¸ì›Œí¬")
    st.caption(f" í˜„ì¬ ì„¤ì •ëœ ìµœì†Œ ë¹ˆë„: {min_edge} (ì‚¬ì´ë“œë°”ì—ì„œ ì¡°ì ˆ ê°€ëŠ¥)")
    
    edges = []
    for s in sentences:
        for a, b in combinations(s, 2):
            edges.append(tuple(sorted((a, b))))
            
    edge_counts = Counter(edges)
    final_edges = [(a, b, c) for (a, b), c in edge_counts.items() if c >= min_edge]

    if final_edges:
        G = nx.Graph()
        G.add_weighted_edges_from(final_edges)
        
        
        fig4, ax4 = plt.subplots(figsize=(15, 15))
        
        # k=1.5ë¡œ ë…¸ë“œ ê°„ê²©ì„ ë„“í˜
        pos = nx.spring_layout(G, k=1.5, iterations=50, seed=42)
        
        d = dict(G.degree)
        node_size = [v * 120 for v in d.values()] # ë…¸ë“œ í¬ê¸° í™•ëŒ€
        
        # ë…¸ë“œ
        nx.draw_networkx_nodes(G, pos, node_size=node_size, node_color='skyblue', alpha=0.9)
        
        # ì—£ì§€ 
        nx.draw_networkx_edges(G, pos, alpha=0.3, edge_color='gray', width=1.0)
        
        # ë¼ë²¨ 
        nx.draw_networkx_labels(G, pos, font_family=font_name, font_size=12, font_weight='bold')
        
        plt.axis('off')
        st.pyplot(fig4)
        
        st.success("""
        **ğŸ‘‰ ë„¤íŠ¸ì›Œí¬ ì‹œê°í™” í•´ì„:**
        * **ë…¸ë“œ (Node, ì ):** ì¶”ì¶œëœ í•µì‹¬ ëª…ì‚¬ì…ë‹ˆë‹¤. ì ì´ í´ìˆ˜ë¡ ë§ì€ ë‹¨ì–´ì™€ ì—°ê²°ëœ **'ì¤‘ì‹¬(Degree Centrality)'** í‚¤ì›Œë“œì…ë‹ˆë‹¤.
        * **ì—£ì§€ (Edge, ì„ ):** ë‘ ë‹¨ì–´ê°€ ê°™ì€ ë¬¸ì¥ì—ì„œ í•¨ê»˜ ë“±ì¥í•œ **'ë™ì‹œ ì¶œí˜„(Co-occurrence)'** ê´€ê³„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
        * **ê²°ë¡ :** ë„¤íŠ¸ì›Œí¬ ì¤‘ì•™ì— ë°€ì§‘ë˜ì–´ ì„œë¡œ ë³µì¡í•˜ê²Œ ì—°ê²°ëœ ë‹¨ì–´ë“¤ì´ ì´ë²ˆ ì´ìŠˆë¥¼ ê´€í†µí•˜ëŠ” í•µì‹¬ ì£¼ì œì…ë‹ˆë‹¤.
        """)
    else:
        st.warning("ì—°ê²°ëœ í‚¤ì›Œë“œê°€ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œë°”ì—ì„œ 'ìµœì†Œ ë¹ˆë„'ë¥¼ ë‚®ì¶°ë³´ì„¸ìš”.")

else:
    st.info("ğŸ‘ˆ ì™¼ìª½ ì‚¬ì´ë“œë°”ì—ì„œ 'ë°ì´í„° ìˆ˜ì§‘ ë° ë¶„ì„ ì‹œì‘' ë²„íŠ¼ì„ ëˆŒëŸ¬ì£¼ì„¸ìš”.")